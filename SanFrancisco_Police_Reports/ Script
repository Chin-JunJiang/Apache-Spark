# Databricks notebook source
# MAGIC %md
# MAGIC 
# MAGIC ## San Francisco Police Reports

# COMMAND ----------

# MAGIC %md
# MAGIC ### Ingestion and Transformation

# COMMAND ----------

from pyspark.sql.types import *
from pyspark.sql.functions import *

df_schema = StructType([StructField("Pdid",StringType() , True),
                       StructField("IncidntNum",IntegerType() , True),
                       StructField("IncidentCode",IntegerType() , True),
                       StructField("Category",StringType() , True),
                       StructField("Descript",StringType() , True),
                       StructField("DayOfWeek",StringType() , True),
                       StructField("Date",StringType() , True),
                       StructField("Time",StringType() , True),
                       StructField("PdDistrict",StringType() , True),
                       StructField("Resolution",StringType() , True),
                       StructField("Address",StringType() , True),
                       StructField("X",StringType() , True),
                       StructField("Y",StringType() , True),
                       StructField("location",StringType() , True)
                       ])

#file location and type
file_location = "/FileStore/tables/Police_Department_Incident_Reports__Historical_2003_to_May_2018.csv"
file_type = "csv"

# CSV options
infer_schema = "false"
first_row_is_header = "True"
delimiter = ","

# The applied options are for CSV files. For other file types, these will be ignored.
df = (spark.read.format(file_type) 
  .option("inferSchema", infer_schema) 
  .option("header", first_row_is_header) 
  .option("sep", delimiter) 
  .schema(df_schema)
  .load(file_location))

df.show(20)

# COMMAND ----------

df.columns

# COMMAND ----------

df.count()

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### There are a total of 2,129,525 reported cases

# COMMAND ----------

#checking for null columns
df_Columns=['Pdid', 'IncidntNum', 'IncidentCode', 'Category', 'Descript', 'DayOfWeek',
            'Date', 'Time', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y', 'location']
df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_Columns]).show()

# COMMAND ----------

# MAGIC %md 
# MAGIC ##### There are no null rows

# COMMAND ----------

from_time_pattern_1 = 'MM/dd/yyyy'
to_time_pattern_1 = 'dd//MM/yyyy'


df = df.withColumn('Date_new',unix_timestamp(df['Date'], from_time_pattern_1).cast('timestamp')) \
        .drop('Date')

# COMMAND ----------

df.show()

# COMMAND ----------

# MAGIC %md 
# MAGIC ### Data Exploration

# COMMAND ----------

#there are 18 different main categories of police reports
df.select('Category').distinct().show(50,False)

# COMMAND ----------

#Larveny/theft is the top reported crime - 477,975 out of 2,129,525 reported cases - roughly about 22%
#Other offenses is the second most reported crime -301,874 out of 2,129,525 reported cases  - roughly about 14%
#Non offenses is the third most reported crime - 236,928 out of 2,129,525 reported cases  - roughly about 11.1%
display(df.select('Category').groupby('Category').count().orderBy("count", ascending=False))

# COMMAND ----------

#GRAND THEFT FROM LOCKED AUTO(178776), PETTY THEFT FROM LOCKED AUTO (51912), PETTY THEFT OF PROPERTY (46049), GRAND THEFT OF PROPERTY(29400) etc
display(df.filter(df.Category=='LARCENY/THEFT').select('Descript')
        .groupBy('Descript').count().orderBy('count',ascending = False))

# COMMAND ----------

#other offenses includes DRIVERS LICENSE, SUSPENDED OR REVOKED(62372), TRAFFIC VIOLATION (37569), RESISTING ARREST (20485), MISCELLANEOUS INVESTIGATION (19901) etc
display(df.filter(df.Category=='OTHER OFFENSES').select('Descript')
        .groupBy('Descript').count().orderBy('count',ascending = False))

# COMMAND ----------

#non crimmal LOST PROPERTY(77947), AIDED CASE-MENTAL DISTURBED (56203), FOUND PROPERTY(33344),AIDED CASE(14088) etc
display(df.filter(df.Category=='NON-CRIMINAL').select('Descript')
        .groupBy('Descript').count().orderBy('count',ascending = False))

# COMMAND ----------

#the report numbers has been consistently in the 140,000 to 150,000 range, with a consistent increment from 2011 to 2015
display(df.select(year('Date_new'),'category').groupBy('year(Date_new)').count().orderBy('count',ascending= False))

# COMMAND ----------

#southern PD District has the highest count of reported crimes - 390,625
display(df.select('PdDistrict').groupBy('PdDistrict').count().orderBy("count",ascending = False))

# COMMAND ----------

#diving deeper into categories of police reports in the SOUTHERN District
display(df.filter(df.PdDistrict=='SOUTHERN').select('category').groupBy('category').count().orderBy("count",ascending = False))

# COMMAND ----------

#checking the number of rdd partitions 
#when you put dataframes in memory, sweet spot for each partition should be 50 MBytes to 200MBytes in memory
df.rdd.getNumPartitions()

# COMMAND ----------

# MAGIC %md
# MAGIC ### Caching of Dataframe

# COMMAND ----------

df.createOrReplaceTempView("df_view")

# COMMAND ----------

spark.catalog.cacheTable("df_view")

# COMMAND ----------

spark.catalog.isCached('df_view')

# COMMAND ----------

#calling count to materialise the caching of table becasue the isCached is a lazy transformation
spark.table('df_view').count()

# COMMAND ----------

df_cached = spark.table('df_view')

# COMMAND ----------

#counting of rows for the chached table which took 0.41 seconds as compared to the countring of rows for the dataframe which took 9.15 seconds
df_cached.count()

# COMMAND ----------

# MAGIC %md
# MAGIC ## SQL Queries

# COMMAND ----------

spark.sql('SELECT * FROM df_view').show()

# COMMAND ----------

#counting the reported crimes for each year
Count_of_Reported_Crimes_by_Year = 
            spark.sql('SELECT YEAR(Date_new) AS YEAR, \
            COUNT(Pdid) AS Count_of_Reported_Crimes_by_Year \
            FROM df_view GROUP BY YEAR ORDER BY YEAR ASC').show(truncate=False)

Count_of_Reported_Crimes_by_Year.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879 \
                                                          -s3-root-bucket/singapore-prod/490048767014810/tmp/ \
                                                          SF_Police_Reports_CSV/Count_of_Reported_Crimes_by_Year"
                                                          , header='true')

# COMMAND ----------

#counting the number of reports based on categories
Count_of_Categorical_Crimes = spark.sql('SELECT Category, \
                                COUNT(Category) AS Count_of_Categorical_Crimes \
                                FROM df_view GROUP BY Category ORDER BY COUNT(Category) DESC') \
                                .show(truncate=False)

Count_of_Categorical_Crimes.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879-s3-root-bucket/ \
                                                     singapore-prod/490048767014810/tmp/SF_Police_Reports_CSV/ \
                                                     Count_of_Categorical_Crimes", header='true')

# COMMAND ----------

#showing in descending order for the described crime committed across the categories of felonies
Counts_of_Described_Commited_Felonies_under_a_Category = spark.sql('SELECT Category, Descript, COUNT(Descript) \
                                                            AS Counts_of_Detailed_Commited_Felonies_under_a_Category \
                                                            FROM df_view GROUP BY Category,Descript \
                                                            ORDER BY COUNT(Descript) DESC').show(truncate=False)

Counts_of_Described_Commited_Felonies_under_a_Category.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879\
                                                                                -s3-root-bucket/singapore-prod/490048767014810/tmp/SF_Police_Reports_CSV/Counts_of_Described_Commited_Felonies_under_a_Category", header='true')

# COMMAND ----------

#showing the police districts in descending order based on the number of felonies committed
count_of_crimes_reported_by_PdDistrict = spark.sql('SELECT PdDistrict, COUNT(PdDistrict) AS Count_of_Felonies_commited \
                                            FROM df_view GROUP BY PdDistrict ORDER BY COUNT(PdDistrict) DESC')
                                            .show(truncate=False)

count_of_crimes_reported_by_PdDistrict.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879-s3-root-bucket/singapore-prod/490048767014810/tmp/SF_Police_Reports_CSV/count_of_crimes_reported_by_PdDistrict", header='true')

# COMMAND ----------

#showing the police districts most commited felonies in descending order based on the number of desscribed felonies committed
Count_of_Described_Felonies_committed_Based_on_PdDistrict = spark.sql('SELECT PdDistrict, Descript , \
                                                            COUNT(Descript) AS Count_of_Described_Felonies_committed \
                                                            FROM df_view GROUP BY PdDistrict,Descript \
                                                            ORDER BY COUNT(Descript) DESC').show(truncate=False)

Count_of_Described_Felonies_committed_Based_on_PdDistrict.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879- \
                                                                                    s3-root-bucket/singapore-prod/490048767014810/tmp/SF_Police_Reports_CSV/Count_of_Described_Felonies_committed_Based_on_PdDistrict", header='true')

# COMMAND ----------

# Total Count of resolved Cases out of total cases
Percentage_of_resolved_cases = spark.sql('SELECT COUNT(RESOLUTION) AS Total_Number_of_Cases, \
                                A.Resolved_Cases AS Total_Number_of_Resolved_Cases \
                               , ROUND(COUNT(RESOLUTION)/A.Resolved_Cases,2) AS Percentage_of_Resolved_Cases \
                               FROM df_view , (SELECT COUNT(RESOLUTION) AS Resolved_Cases \
                               FROM df_view WHERE Resolution != "NONE" ) AS A GROUP BY A.Resolved_Cases ').show(truncate=False)

Percentage_of_resolved_cases.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879-s3-root-bucket/ \
singapore-prod/490048767014810/tmp/SF_Police_Reports_CSV/Percentage_of_resolved_cases", header='true')

# COMMAND ----------

# counts for each resolution type
Counts_of_resolution_types = spark.sql('SELECT DISTINCT Resolution,COUNT(RESOLUTION) AS Resolution_Type \
                                FROM df_view GROUP BY RESOLUTION ORDER BY Resolution_Type DESC')
                                .show(truncate=False)

Counts_of_resolution_types.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879-s3-root-bucket/singapore-prod/490048767014810/tmp/SF_Police_Reports_CSV/Counts_of_resolution_types", header='true')

# COMMAND ----------

#checking the descriptions of felonies for None resolution
#these crimes were reported but the perpertrators where not found
Count_of_Cases_With_no_Resolution_based_on_Described_crime = spark.sql('SELECT DISTINCT Descript, \
                                                                COUNT(Descript) AS Count_of_Cases_With_no_Resolution \
                                                                FROM df_view WHERE Resolution == "NONE" \
                                                                GROUP BY Descript ORDER BY Count_of_Cases_With_no_Resolution DESC')
                                                                .show(truncate=False)

Count_of_Cases_With_no_Resolution_based_on_Described_crime.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879 \
                                                                                    -s3-root-bucket/singapore-prod/490048767014810/tmp/SF_Police_Reports_CSV/Count_of_Cases_With_no_Resolution_based_on_Described_crime", header='true')

# COMMAND ----------

#counts of reports from detailed addresses based on year
counts_of_reports_from_detailed_addresses_based_on_year = spark.sql('SELECT Address,YEAR(Date_new) AS Year, \
                                                           COUNT(Address) AS Count_of_Reports_from_a_detailed_address \
                                                           FROM df_view GROUP BY Address,YEAR(Date_new) \
                                                           ORDER BY Count_of_Reports_from_a_detailed_address DESC \
                                                           ,YEAR(Date_new) DESC,Address').show(truncate=False)

counts_of_reports_from_detailed_addresses_based_on_year.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879-s3-root-bucket/singapore-prod/490048767014810/tmp/SF_Police_Reports_CSV/counts_of_reports_from_detailed_addresses_based_on_year", header='true')

# COMMAND ----------

#count of reported crimes based on day
count_of_reported_crimes_based_on_day = spark.sql('SELECT DayOfWeek, COUNT(Pdid) AS Count_of_Reported_Crimes \
                                        FROM df_view GROUP BY DayOfWeek ORDER BY Count_of_Reported_Crimes DESC')
                                        .show(truncate=False)

count_of_reported_crimes_based_on_day.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879-s3-root-bucket/singapore-prod/490048767014810/tmp/SF_Police_Reports_CSV/count_of_reported_crimes_based_on_day", header='true')

# COMMAND ----------

#count of described crimes based on day
count_of_described_crimes_based_on_day = spark.sql('SELECT DayOfWeek,Category, \
                                         COUNT(Pdid) AS Count_of_Reported_Crimes \
                                         FROM df_view GROUP BY DayOfWeek,Category \ 
                                         ORDER BY DayOfWeek,Count_of_Reported_Crimes DESC')
                                         .show(truncate=False)

count_of_described_crimes_based_on_day.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879-s3-root-bucket/singapore-prod/490048767014810/tmp/SF_Police_Reports_CSV/count_of_described_crimes_based_on_day", header='true')

# COMMAND ----------

#count of crimes reported based on hour mark
count_of_crimes_reported_based_on_hour_mark = spark.sql('SELECT LEFT(Time,2) AS HOUR_MARK, \
                                                COUNT(Pdid) AS Number_of_Crimes_reported \
                                                FROM df_view GROUP BY HOUR_MARK \
                                                ORDER BY Number_of_Crimes_reported DESC').show(truncate=False)

count_of_crimes_reported_based_on_hour_mark.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879-s3-root-bucket/singapore-prod/490048767014810/tmp/SF_Police_Reports_CSV/count_of_crimes_reported_based_on_hour_mark", header='true')

# COMMAND ----------

#count of categorised crimes reported based on hour mark
count_of_categorised_crimes_reported_based_on_hour_mark = spark.sql('SELECT LEFT(Time,2) AS Hour_Mark,Category, \
                                                           COUNT(Pdid)  AS Number_of_Crimes_reported \
                                                           FROM df_view GROUP BY Hour_Mark,Category \
                                                           ORDER BY Hour_Mark,Number_of_Crimes_reported DESC').show(truncate=False)

count_of_categorised_crimes_reported_based_on_hour_mark.write.format('csv').save("s3://db-1422b887d4757cac19d76c2be12cc879-s3-root-bucket/singapore-prod/490048767014810/tmp/SF_Police_Reports_CSV/count_of_categorised_crimes_reported_based_on_hour_mark", header='true')

# COMMAND ----------

# MAGIC %fs ls 

# COMMAND ----------

#writing the dataframe into parquet format to S3 bucket as parquet format is much more efficient than csv which allows faster reading of files
#ensure all column names do not have spaces or unique symbols
df.write.format('parquet').save("s3://db-1422b887d4757cac19d76c2be12cc879-s3-root-bucket/singapore-prod/490048767014810/tmp/SFPoliceReports1.parquet")